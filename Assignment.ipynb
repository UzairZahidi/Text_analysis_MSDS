{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ff3a8c-4cba-4d26-8873-1e73f9aec3c5",
   "metadata": {},
   "source": [
    "# Text Analytics Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c0a93-258b-48ef-85b4-6ab0dfa0bef7",
   "metadata": {},
   "source": [
    "# Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb73cf-a399-421a-8e70-367ad4039909",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d380c4b-d11b-4976-8066-bf5c4089f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb0475-ce4d-4313-ae8d-206031f6b6c8",
   "metadata": {},
   "source": [
    "### Loading data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce13ef7e-c555-461a-9505-f662c8aee21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and preprocess the headlines\n",
    "data = pd.read_csv(\"news.csv\")\n",
    "\n",
    "# Set up the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocess the headlines\n",
    "preprocessed_headlines = []\n",
    "for headline in data['headline']:\n",
    "    # Tokenize the headline\n",
    "    tokens = word_tokenize(headline.lower())\n",
    "\n",
    "    # Remove stop words and stem the tokens\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Add the preprocessed headline to the list\n",
    "    preprocessed_headlines.append(\" \".join(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64a4f7-df6e-4361-9d6f-4ddfe92a0292",
   "metadata": {},
   "source": [
    "### Training Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c176b4f4-8f2d-4d64-a2e4-f01d1e3bd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(preprocessed_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ca30ae8-0300-4a47-8b0c-2fd667c88e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bow(given_headline):\n",
    "    tokens = word_tokenize(given_headline.lower())\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    preprocessed_given_headline = \" \".join(tokens)\n",
    "    given_headline_bow = vectorizer.transform([preprocessed_given_headline])\n",
    "    return given_headline_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "170c87c7-eaa7-4181-b0cb-8cbad2dc52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar headlines (bag-of-words with stemming):\n",
      "Ariana Grande Drops Wise Feminist Truths On Twitter\n",
      "Britney Is Thoroughly Unimpressed With Ariana Grande's Impression\n",
      "Ariana Grande Heads To Baltimore For NBC's 'Hairspray Live'\n",
      "Ariana Grande Takes A Tumble At The Billboard Music Awards\n",
      "Ariana Grande Issues 'Donut Fiasco' Apology Video That Doesn't Explain Donut-Licking\n"
     ]
    }
   ],
   "source": [
    "input1 = preprocess_bow(\"Ariana Grande Drops Wise Feminist Truths On\")\n",
    "\n",
    "cosine_similarities = cosine_similarity(input1, bow_matrix)[0]\n",
    "most_similar_headline_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "\n",
    "print(\"Top 5 similar headlines (bag-of-words with stemming):\")\n",
    "for index in most_similar_headline_indices:\n",
    "    print(data.iloc[index]['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03f6eb-f113-4d62-990c-85a9393cc0fd",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2964d-9512-49e5-949d-9ee7348c2811",
   "metadata": {},
   "source": [
    "### Importing Libraries loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a865d38d-2860-40e3-82e5-6b5e623e673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the CSV file and preprocess the headlines\n",
    "data = pd.read_csv(\"news.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16665c-0708-42ab-ad1d-f7a52d6df05f",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e37df295-ccb1-4cc3-90fe-731043fae56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the headlines\n",
    "preprocessed_headlines = []\n",
    "for headline in data['headline']:\n",
    "    # Tokenize the headline\n",
    "    tokens = word_tokenize(headline.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Add the preprocessed headline to the list\n",
    "    preprocessed_headlines.append(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2726d-d49e-4fc6-be67-d0196519a209",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a50aa2b1-ea03-47a2-b500-d1155e011498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar headlines (Word2Vec with cosine similarity):\n",
      "Ariana Grande Takes A Tumble At The Billboard Music Awards\n",
      "Ariana Grande Issues 'Donut Fiasco' Apology Video That Doesn't Explain Donut-Licking\n",
      "Ariana Grande Heads To Baltimore For NBC's 'Hairspray Live'\n",
      "Princess Diana Challenged Postpartum Depression Stigma Over 20 Years Ago\n",
      "Creative Community Taking Women's March to Sundance\n"
     ]
    }
   ],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(preprocessed_headlines, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find the top 5 most similar headlines to a given headline using the Word2Vec approach and Cosine Similarity\n",
    "given_headline = \"Ariana Grande Drops Wise Feminist\"\n",
    "given_headline_tokens = word_tokenize(given_headline.lower())\n",
    "given_headline_tokens = [token for token in given_headline_tokens if token not in stop_words]\n",
    "given_headline_embedding = model.wv[given_headline_tokens]\n",
    "\n",
    "similar_headlines = []\n",
    "for i, headline in enumerate(preprocessed_headlines):\n",
    "    headline_embedding = model.wv[headline]\n",
    "    similarity = cosine_similarity(given_headline_embedding, headline_embedding)\n",
    "    similar_headlines.append((i, similarity[0][0]))\n",
    "\n",
    "# Sort the similar headlines by similarity score and take the top 5\n",
    "similar_headlines.sort(key=lambda x: x[1], reverse=True)\n",
    "top_5_headlines = similar_headlines[1:6]\n",
    "\n",
    "print(\"Top 5 similar headlines (Word2Vec with cosine similarity):\")\n",
    "for headline in top_5_headlines:\n",
    "    print(data.iloc[headline[0]]['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e806e05-022f-43e1-915a-e05699efb320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706650c6-d889-4c15-8749-11f72dc22567",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc99f4-ca72-4bc1-8f0d-85962abb5e7a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcba4ab3-332a-4ba2-9d90-b4554d340824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa31af7-1da6-4cec-82c3-762b2b279cbf",
   "metadata": {},
   "source": [
    "### Loading Data and preprpcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a2bee6b-6487-41e0-8148-96bab2ac3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and preprocess the headlines\n",
    "data = pd.read_csv(\"news.csv\")\n",
    "\n",
    "# Preprocess the headlines\n",
    "preprocessed_headlines = []\n",
    "for headline in data['headline']:\n",
    "    # Tokenize the headline\n",
    "    tokens = word_tokenize(headline.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Add the preprocessed headline to the list\n",
    "    preprocessed_headlines.append(tokens)\n",
    "\n",
    "# Load the pre-trained GloVe word embeddings\n",
    "glove_path = \"glove.6B.50d.txt\"\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        embedding = np.array([float(val) for val in values[1:]])\n",
    "        glove_embeddings[word] = embedding\n",
    "\n",
    "# Compute the embeddings for each headline\n",
    "embedding_size = len(glove_embeddings['the'])\n",
    "headline_embeddings = []\n",
    "for headline in preprocessed_headlines:\n",
    "    headline_embedding = np.zeros(embedding_size)\n",
    "    num_tokens = 0\n",
    "    for token in headline:\n",
    "        if token in glove_embeddings:\n",
    "            headline_embedding += glove_embeddings[token]\n",
    "            num_tokens += 1\n",
    "    if num_tokens > 0:\n",
    "        headline_embedding /= num_tokens\n",
    "    headline_embeddings.append(headline_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3856f68-378d-4cfa-9975-fec27e0d023d",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eefc24eb-96b7-41a9-a87f-f4ab694d7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar headlines (GloVe with cosine similarity):\n",
      "Is My Hairstyle a Feminist Statement?\n",
      "Britney Is Thoroughly Unimpressed With Ariana Grande's Impression\n",
      "Ariana Grande Issues 'Donut Fiasco' Apology Video That Doesn't Explain Donut-Licking\n",
      "This 'La La Land'-esque YouTube Love Story Has Us All Choked Up\n",
      "The Writing Life: Elections and Intuition\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 most similar headlines to a given headline using GloVe embeddings and Cosine Similarity\n",
    "given_headline = \"Ariana Grande Drops Wise Feminist Truths On\"\n",
    "given_headline_tokens = word_tokenize(given_headline.lower())\n",
    "given_headline_tokens = [token for token in given_headline_tokens if token not in stop_words]\n",
    "given_headline_embedding = np.zeros(embedding_size)\n",
    "num_tokens = 0\n",
    "for token in given_headline_tokens:\n",
    "    if token in glove_embeddings:\n",
    "        given_headline_embedding += glove_embeddings[token]\n",
    "        num_tokens += 1\n",
    "if num_tokens > 0:\n",
    "    given_headline_embedding /= num_tokens\n",
    "\n",
    "similar_headlines = []\n",
    "for i, headline_embedding in enumerate(headline_embeddings):\n",
    "    if np.any(headline_embedding):\n",
    "        similarity = cosine_similarity([given_headline_embedding], [headline_embedding])\n",
    "        similar_headlines.append((i, similarity[0][0]))\n",
    "\n",
    "# Sort the similar headlines by similarity score and take the top 5\n",
    "similar_headlines.sort(key=lambda x: x[1], reverse=True)\n",
    "top_5_headlines = similar_headlines[1:6]\n",
    "\n",
    "print(\"Top 5 similar headlines (GloVe with cosine similarity):\")\n",
    "for headline in top_5_headlines:\n",
    "    print(data.iloc[headline[0]]['headline'])\n",
    "    #print(headline[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a1230-846a-4128-a1ec-1b98207423e2",
   "metadata": {},
   "source": [
    "# SVD & LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdeb7e-3946-4226-a301-b26f137c0600",
   "metadata": {},
   "source": [
    "### Importing Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05d87052-98ed-4702-b66b-c235a7a736fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056b52a-be57-4d1e-9afa-f7e0b1618ea3",
   "metadata": {},
   "source": [
    "### Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "089705c6-c9f8-4a7b-8eef-6e40dae2a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('news.csv')\n",
    "\n",
    "# Preprocess data\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['headline_clean'] = df['headline'].apply(preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb02497-bdb0-4303-8aca-158249a28e1b",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "859a952c-2a16-4cd5-8d6b-2afb62a9f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(df['headline_clean'])\n",
    "\n",
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "\n",
    "# Find similar headlines\n",
    "def find_similar_headlines(query, num_results=5):\n",
    "    query_dtm = vectorizer.transform([preprocess(query)])\n",
    "    query_lsa = lsa.transform(query_dtm)\n",
    "    cosine_similarities = np.dot(query_lsa, dtm_lsa.T)\n",
    "    similar_indices = cosine_similarities.argsort()[0][-num_results-1:-1][::-1]\n",
    "    return df.iloc[similar_indices]['headline'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418f0f4-aa5d-48ce-add4-55a9f16faec0",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c5760ed-6618-4f05-9278-504eee61e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar headlines:\n",
      " ['Ariana Grande Drops Wise Feminist Truths On Twitter'\n",
      " 'Is My Hairstyle a Feminist Statement?'\n",
      " \"Ariana Grande Heads To Baltimore For NBC's 'Hairspray Live'\"\n",
      " 'Ariana Grande Takes A Tumble At The Billboard Music Awards'\n",
      " \"Ariana Grande Issues 'Donut Fiasco' Apology Video That Doesn't Explain Donut-Licking\"]\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "input_headline = \"Ariana Grande Drops Wise Feminist\"\n",
    "similar_headlines = find_similar_headlines(input_headline)\n",
    "print(\"Similar headlines:\\n\", similar_headlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469304f-135e-4c98-8838-dc0e4e2a79de",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ec057-7c59-4176-86a6-b368b142525d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
